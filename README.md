# Demo FastAPI WebSocket Audio
Web audio --WebSocket--> FastAPI Server.

## run
```
uvicorn src.main:app --reload
```

## Web Audio Concepts and usage

The API is based on the manipulation of a [`MediaStream`](https://developer.mozilla.org/en-US/docs/Web/API/MediaStream) object representing a flux of audio- or video-related data. See an example in [Get the video](https://developer.mozilla.org/en-US/docs/WebRTC/taking_webcam_photos#Get_the_video).

A `MediaStream` consists of zero or more [`MediaStreamTrack`](https://developer.mozilla.org/en-US/docs/Web/API/MediaStreamTrack) objects, representing various audio or video **tracks**. Each `MediaStreamTrack` may have one or more **channels**. The channel represents the smallest unit of a media stream, such as an audio signal associated with a given speaker, like *left* or *right* in a stereo audio track.

`MediaStream` objects have a single **input** and a single **output**. A `MediaStream` object generated by [`getUserMedia()`](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia) is called *local*, and has as its source input one of the user's cameras or microphones. A non-local `MediaStream` may be representing to a media element, like [``](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/video) or [``](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/audio), a stream originating over the network, and obtained via the WebRTC [`RTCPeerConnection`](https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection) API, or a stream created using the [Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API) [`MediaStreamAudioSourceNode`](https://developer.mozilla.org/en-US/docs/Web/API/MediaStreamAudioSourceNode).

The output of the `MediaStream` object is linked to a **consumer**. It can be a media elements, like [``](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/audio) or [``](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/video), the WebRTC [`RTCPeerConnection`](https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection) API or a [Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API) [`MediaStreamAudioSourceNode`](https://developer.mozilla.org/en-US/docs/Web/API/MediaStreamAudioSourceNode).

https://developer.mozilla.org/en-US/docs/Web/API/Media_Streams_API

## Access the raw data from the microphone

- `navigator.mediaDevices.getUserMedia`：for read microphone stream.
- `context.createScriptProcessor`: for process audio buffer, though it is deprecated.

```
const handleSuccess = function (stream) {
    const context = new AudioContext();
    const source = context.createMediaStreamSource(stream);
    const processor = context.createScriptProcessor(1024, 1, 1);

    source.connect(processor);
    processor.connect(context.destination);

    processor.onaudioprocess = function (e) {
        // Do something with the data, e.g. convert it to WAV
        console.log(e.inputBuffer);
    };
};

navigator.mediaDevices.getUserMedia({ audio: true, video: false })
    .then(handleSuccess);
```

https://developers.google.com/web/fundamentals/media/recording-audio#access_the_raw_data_from_the_microphone

## Two audio resample method
Cause getUserMedia with Constraint Not work, so resample by the following methods:  
### Use OfflineAudioContext(native code)

```
// `sourceAudioBuffer` is an AudioBuffer instance of the source audio
// at the original sample rate.
const DESIRED_SAMPLE_RATE = 16000;
const offlineCtx = new OfflineAudioContext(sourceAudioBuffer.numberOfChannels, sourceAudioBuffer.duration * DESIRED_SAMPLE_RATE, DESIRED_SAMPLE_RATE);
const cloneBuffer = offlineCtx.createBuffer(sourceAudioBuffer.numberOfChannels, sourceAudioBuffer.length, sourceAudioBuffer.sampleRate);
// Copy the source data into the offline AudioBuffer
for (let channel = 0; channel < sourceAudioBuffer.numberOfChannels; channel++) {
    cloneBuffer.copyToChannel(sourceAudioBuffer.getChannelData(channel), channel);
}
// Play it from the beginning.
const source = offlineCtx.createBufferSource();
source.buffer = cloneBuffer;
source.connect(offlineCtx.destination);
offlineCtx.oncomplete = function (e) {
    // `resampledAudioBuffer` contains an AudioBuffer resampled at 16000Hz.
    // use resampled.getChannelData(x) to get an Float32Array for channel x.
    const resampledAudioBuffer = e.renderedBuffer;
    console.log(resampledAudioBuffer);
}
offlineCtx.startRendering();
source.start(0);
```

https://stackoverflow.com/a/55427982/974526  
### Use javascript code Resampler

```
navigator.mediaDevices.getUserMedia({audio: true})
    .then((stream) => {
        let context = new AudioContext(),
            bufSize = 4096,
            microphone = context.createMediaStreamSource(stream),
            processor = context.createScriptProcessor(bufSize, 1, 1),
            res = new Resampler(context.sampleRate, 16000, 1, bufSize),
            bufferArray = [];

        processor.onaudioprocess = (event) => {
            console.log('onaudioprocess');
            // const right = event.inputBuffer.getChannelData(1);
            const outBuf = res.resample(event.inputBuffer.getChannelData(0));
            bufferArray.push.apply(bufferArray, outBuf);
        }
    }
}
```

https://github.com/felix307253927/resampler

## Why Constraints Not work

Although `navigator.mediaDevices.getUserMedia` is set by following MediaTrackConstraints: `mediaStreamConstraints`, the stream is still at SampleRate 48000. **Because** the Chrome browser I use only support sampleRate 48000.

```
const mediaStreamConstraints = {
   audio: {
     channelCount: 1,
     sampleRate: 16000,
     sampleSize: 16
   }
}
// set constraints at begining
navigator.mediaDevices.getUserMedia(mediaStreamConstraints)
 .catch( err => serverlog(`ERROR mediaDevices.getUserMedia: ${err}`) )
 .then( stream => {
     const track = mediaStream.getAudioTracks()[0];
     // can update audio track Constraints here
     // track.applyConstraints(mediaStreamConstraints['audio'])
     .then(() => {
       console.log(track.getCapabilities());
     });
     
    // audio recorded as Blob 
    // and the binary data are sent via socketio to a nodejs server
    // that store blob as a file (e.g. audio/inp/audiofile.webm)
  } )
```
So **how to check the capabilities?**

```
let stream = await navigator.mediaDevices.getUserMedia({audio: true});
let track = stream.getAudioTracks()[0];
console.log(track.getCapabilities());
```

output:

```
{autoGainControl: Array(2), channelCount: {…}, deviceId: "default", echoCancellation: Array(2), groupId: "1e76386ad54f9ad3548f6f6c14c08e7eff6753f9362d93d8620cc48f546604f5", …}
autoGainControl: (2) [true, false]
channelCount: {max: 2, min: 1}
deviceId: "default"
echoCancellation: (2) [true, false]
groupId: "1e76386ad54f9ad3548f6f6c14c08e7eff6753f9362d93d8620cc48f546604f5"
latency: {max: 0.01, min: 0.01}
noiseSuppression: (2) [true, false]
sampleRate: {max: 48000, min: 48000}
sampleSize: {max: 16, min: 16}
__proto__: Object
```

https://developer.mozilla.org/en-US/docs/Web/API/Media_Streams_API/Constraints